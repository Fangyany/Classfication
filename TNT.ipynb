{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from fractions import gcd\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from data import ArgoDataset, collate_fn\n",
    "from utils import gpu, to_long,  Optimizer, StepLR\n",
    "\n",
    "from layers import Conv1d, Res1d, Linear, LinearRes, Null\n",
    "from numpy import float64, ndarray\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from lanegcn import PredNet, get_model\n",
    "import torch\n",
    "from torch.utils.data import Sampler, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "config, Dataset, collate_fn, net, loss, post_process, opt = get_model()\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "from importlib import import_module\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Sampler, DataLoader\n",
    "\n",
    "\n",
    "from utils import Logger, load_pretrain\n",
    "def worker_init_fn(pid):\n",
    "    np_seed = int(pid)\n",
    "    np.random.seed(np_seed)\n",
    "    random_seed = np.random.randint(2 ** 32 - 1)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "\n",
    "dataset = Dataset(config[\"train_split\"], config, train=True)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=config[\"workers\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['city', 'orig', 'gt_preds', 'has_preds', 'theta', 'rot', 'feats', 'ctrs', 'graph', 'trajs2', 'traj1'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lanegcn import ActorNet, MapNet, actor_gather, graph_gather\n",
    "actor_net = ActorNet(config)\n",
    "map_net = MapNet(config)\n",
    "\n",
    "# construct actor feature\n",
    "actors, actor_idcs = actor_gather(gpu(data[\"feats\"]))\n",
    "actor_ctrs = gpu(data[\"ctrs\"])\n",
    "actors = actor_net(actors)\n",
    "\n",
    "# construct map features\n",
    "graph = graph_gather(to_long(gpu(data[\"graph\"])))\n",
    "nodes, node_idcs, node_ctrs = map_net(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, d_k, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / self.d_k\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_x, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_x, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_x, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_x, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_x, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_k=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_x, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        batch, len_x = x.size(0), x.size(1)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        # Pass through the pre-attention projection: b x len_x x (n*d_v)\n",
    "        # Separate different heads: b x len_x x n x d_v\n",
    "        q = self.w_qs(x).view(batch, len_x, n_head, d_k)\n",
    "        k = self.w_ks(x).view(batch, len_x, n_head, d_k)\n",
    "        v = self.w_vs(x).view(batch, len_x, n_head, d_v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x len_x x d_v\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        out, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Transpose to move the head dimension back: b x len_x x n x d_v\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x len_x x (n*d_v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, len_x, -1)\n",
    "        out = self.dropout(self.fc(out))\n",
    "        out += residual\n",
    "\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        return out, attn\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)  # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)  # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttnEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_x, d_k, d_v, n_head, d_inner, dropout=0.1):\n",
    "        super(MultiHeadAttnEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            n_head, d_x, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_x, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, self_attn_mask=None):\n",
    "        enc_output, enc_self_attn = self.self_attn(\n",
    "            enc_input, mask=self_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output, enc_self_attn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dAggreBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Aaggregation block using max-pooling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_feat: int, dropout: float = 0.0) -> None:\n",
    "        super(Conv1dAggreBlock, self).__init__()\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "        self.n_feat = n_feat\n",
    "\n",
    "        self.conv_1 = Conv1d(n_feat, n_feat, kernel_size=1, norm=norm, ng=ng)\n",
    "        self.conv_2 = Conv1d(n_feat*2, n_feat, kernel_size=1, norm=norm, ng=ng)\n",
    "\n",
    "        self.aggre_func = F.adaptive_avg_pool1d\n",
    "\n",
    "        self.conv_3 = Conv1d(n_feat, n_feat, kernel_size=1, norm=norm, ng=ng, act=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        '''\n",
    "            feats: (batch, c, N)\n",
    "        '''\n",
    "        res = feats\n",
    "        feats = self.conv_1(feats)\n",
    "        feats_mp, _ = feats.max(dim=1)  # global max-pooling\n",
    "\n",
    "        feats_mp = feats_mp.unsqueeze(1).repeat((1, self.n_feat, 1))\n",
    "        feats = torch.cat([feats, feats_mp], dim=1)\n",
    "        feats = self.conv_2(feats)\n",
    "        feats = self.dropout(feats)\n",
    "\n",
    "        feats = self.conv_3(feats)\n",
    "        feats += res\n",
    "        feats = self.relu(feats)\n",
    "\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalDecoder(nn.Module):\n",
    "    def __init__(self, config, n_feat=32, n_pts=200):\n",
    "        super(GoalDecoder, self).__init__()\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.aggre_1 = Conv1dAggreBlock(n_feat=n_feat, dropout=0.1)\n",
    "        self.conv_1 = Conv1d(n_feat, 8, kernel_size=1, norm=norm, ng=ng)\n",
    "\n",
    "        self.aggre_2 = Conv1dAggreBlock(n_feat=8, dropout=0.1)\n",
    "        self.conv_2 = Conv1d(8, 4, kernel_size=1, norm=norm, ng=ng)\n",
    "\n",
    "        self.conv_3 = Conv1d(4, 1, kernel_size=1, norm=norm, ng=ng, act=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, feat, coord):\n",
    "        '''\n",
    "            feat:   (batch, N, n_feat)\n",
    "            coord:  (batch, N, 2)\n",
    "        '''\n",
    "        feat = feat.transpose(1, 2)\n",
    "\n",
    "        feat = self.aggre_1(feat)\n",
    "        feat = self.conv_1(feat)\n",
    "        feat = self.dropout(feat)\n",
    "\n",
    "        feat = self.aggre_2(feat)\n",
    "        feat = self.conv_2(feat)\n",
    "        feat = self.dropout(feat)\n",
    "\n",
    "        feat = self.conv_3(feat)\n",
    "\n",
    "        weights = F.softmax(feat, dim=-1).transpose(1, 2)  # weights, (batch, N, 1)\n",
    "        goal = torch.sum(coord * weights, dim=1)\n",
    "\n",
    "        return goal.unsqueeze(1), weights  # (batch, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_traj_output(x):\n",
    "    # e.g., [batch, seq, 1] or # [batch, n_dec, seq, 1]\n",
    "    muX = x[..., 0:1]  # [..., 1]\n",
    "    muY = x[..., 1:2]  # [..., 1]\n",
    "    sigX = x[..., 2:3]  # [..., 1]\n",
    "    sigY = x[..., 3:4]  # [..., 1]\n",
    "    rho = x[..., 4:5]  # [..., 1]\n",
    "    sigX = torch.exp(sigX)\n",
    "    sigY = torch.exp(sigY)\n",
    "    rho = torch.tanh(rho)\n",
    "\n",
    "    out = torch.cat([muX, muY, sigX, sigY, rho], dim=-1)  # [..., 5]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GodaClassifierDaOnly(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GodaClassifierDaOnly, self).__init__()\n",
    "        n_map = 128\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.fc_1 = Linear(n_map, 16, norm=norm, ng=ng, act=True)\n",
    "        self.fc_2 = Linear(16, 8, norm=norm, ng=ng, act=True)\n",
    "        self.fc_3 = nn.Linear(8, 1, bias=False)\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=config[\"p_dropout\"])\n",
    "\n",
    "    def forward(self, feat):\n",
    "        feat = self.fc_1(feat)\n",
    "        feat = self.fc_2(feat)\n",
    "        # feat = self.dropout(feat)\n",
    "\n",
    "        feat = self.fc_3(feat)\n",
    "        out = torch.sigmoid_(feat).view(-1)\n",
    "        return out\n",
    "\n",
    "class GoalGenerator(nn.Module):\n",
    "    def __init__(self, config, n_blk=2):\n",
    "        super(GoalGenerator, self).__init__()\n",
    "        n_mode = 6\n",
    "        n_feat = 128\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "        self.n_blk = n_blk\n",
    "\n",
    "        self.conv_1 = Conv1d(35, n_feat, kernel_size=1, norm=norm, ng=ng)\n",
    "\n",
    "        self.aggre = nn.ModuleList([\n",
    "            MultiHeadAttnEncoderLayer(d_x=n_feat, d_k=n_feat, d_v=n_feat, n_head=2,\n",
    "                                      d_inner=n_feat, dropout=0.1)\n",
    "            for _ in range(self.n_blk)])\n",
    "\n",
    "        self.multihead_decoder = nn.ModuleList([\n",
    "            GoalDecoder(config=config, n_feat=n_feat, n_pts=200) for _ in range(n_mode)\n",
    "        ])\n",
    "\n",
    "    def forward(self, score, coord, goda_feat):\n",
    "        feat = torch.cat([coord, score.unsqueeze(2), goda_feat], dim=2).transpose(1, 2)  # (batch, 35, N)\n",
    "        feat = self.conv_1(feat)\n",
    "        feat = feat.transpose(1, 2)  # (batch, N, n_feat)\n",
    "\n",
    "        for enc_layer in self.aggre:\n",
    "            feat, _ = enc_layer(feat, self_attn_mask=None)  # (batch, N, n_feat)\n",
    "\n",
    "        goals = []\n",
    "        weights = []\n",
    "        for decoder in self.multihead_decoder:\n",
    "            goals_mode, weights_mode = decoder(feat, coord)\n",
    "            goals.append(goals_mode)\n",
    "            weights.append(weights_mode)\n",
    "        goals = torch.cat(goals, dim=1)  # (batch, n_mode, 2)\n",
    "\n",
    "        return goals\n",
    "\n",
    "class TrajCompletor(nn.Module):\n",
    "    def __init__(self, config, prob_output=True):\n",
    "        super(TrajCompletor, self).__init__()\n",
    "        self.prob_output = prob_output\n",
    "        norm = \"GN\"\n",
    "        ng = 1\n",
    "\n",
    "        self.fc_1 = LinearRes(130, 128, norm=norm, ng=ng)\n",
    "        # self.fc_2 = LinearRes(128, 128, norm=norm, ng=ng)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "        if self.prob_output:\n",
    "            self.fc_d = nn.Linear(128, 30*5, bias=False)\n",
    "        else:\n",
    "            self.fc_d = nn.Linear(128, 30*2, bias=False)\n",
    "\n",
    "    def forward(self, traj_enc, goal):\n",
    "        '''\n",
    "            traj_enc:   (batch, 128)\n",
    "            goal:       (batch, n_mode, 2)\n",
    "        '''\n",
    "        n_batch = goal.shape[0]\n",
    "        n_mode = goal.shape[1]\n",
    "        x = torch.cat([traj_enc.unsqueeze(1).repeat((1, n_mode, 1)), goal], dim=2)\n",
    "\n",
    "        x = x.reshape(-1, 130)\n",
    "        x = self.fc_1(x)\n",
    "        # x = self.fc_2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.prob_output:\n",
    "            traj_pred = self.fc_d(x).reshape(n_batch, n_mode, 30, 5)\n",
    "            traj_pred = prob_traj_output(traj_pred)\n",
    "        else:\n",
    "            traj_pred = self.fc_d(x).reshape(n_batch, n_mode, 30, 2)\n",
    "\n",
    "        return traj_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ decoders\n",
    "goda_classifier = GodaClassifierDaOnly(config)\n",
    "traj_completor = TrajCompletor(config)\n",
    "\n",
    "# ~ final goal generation\n",
    "goal_generator = GoalGenerator(config, n_blk=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_max, scatter_mean, scatter_add\n",
    "device = torch.device('cpu')\n",
    "cfg = config\n",
    "training = True\n",
    "\n",
    "def decode_goal_and_traj(feat_da, graph_da, actors, actor_idcs, goal_gt):\n",
    "    batch_size = len(graph_da['ctrs'])\n",
    "    agent_idcs = torch.LongTensor([idcs[0] for idcs in actor_idcs])\n",
    "    agent_feat = actors[agent_idcs]\n",
    "\n",
    "    goda_cls = goda_classifier(feat_da)\n",
    "\n",
    "    DEFAULT_GODA_NUM = 200\n",
    "    goda_score = []\n",
    "    goda_coord = []\n",
    "    goda_feat = []\n",
    "    for i in range(batch_size):\n",
    "        scores = goda_cls[graph_da['idcs'][i]]\n",
    "        _, idcs = torch.sort(scores, descending=True)\n",
    "        assert len(idcs) > DEFAULT_GODA_NUM, 'Invalid goda number'\n",
    "\n",
    "        idcs = idcs[:DEFAULT_GODA_NUM]\n",
    "        goda_score.append(scores[idcs])\n",
    "        goda_coord.append(graph_da['ctrs'][i][idcs])\n",
    "        goda_feat.append(feat_da[graph_da['idcs'][i]][idcs])\n",
    "\n",
    "    goda_score = torch.stack(goda_score, dim=0)\n",
    "    goda_coord = torch.stack(goda_coord, dim=0)\n",
    "    goda_feat = torch.stack(goda_feat, dim=0)\n",
    "\n",
    "    goal_pred = goal_generator(goda_score, goda_coord, goda_feat)  # (batch, n_mode, 2)\n",
    "\n",
    "    '''\n",
    "        train: use GT goal\n",
    "        test/val: use generated goals\n",
    "    '''\n",
    "    if training:\n",
    "        goal_mock = gpu(torch.stack(goal_gt), device=device).unsqueeze(1)\n",
    "        traj_pred = traj_completor(agent_feat, goal_mock)\n",
    "        traj_pred = traj_pred.repeat((1, cfg['n_mode'], 1, 1))  # [batch, n_mode, len_pred, 2]\n",
    "        score_pred = torch.ones(traj_pred.shape[0], traj_pred.shape[1])\n",
    "    else:\n",
    "        # sum/mean before sampling\n",
    "        edges = graph_da['ms_edges'][0]\n",
    "        goda_cls_tmp = goda_cls.clone()\n",
    "        goda_cls_tmp = scatter_add(goda_cls_tmp.index_select(0, edges['u']), edges['v'], out=goda_cls_tmp, dim=0)\n",
    "\n",
    "        traj_pred = traj_completor(agent_feat, goal_pred)\n",
    "        # ! tmp, overwrite\n",
    "        traj_pred[:, :, -1, 0:2] = goal_pred\n",
    "\n",
    "        # score_pred = torch.ones(traj_pred.shape[0], traj_pred.shape[1])  # uniform distribution\n",
    "        # ~ score from heatmap\n",
    "        score_pred = []\n",
    "        for b in range(batch_size):\n",
    "            goals = graph_da['ctrs'][b]\n",
    "            goal_scores = goda_cls_tmp[graph_da['idcs'][b]]\n",
    "            score_tmp = []\n",
    "            for g in goal_pred[b]:\n",
    "                dist = torch.sqrt(torch.sum((goals - g.unsqueeze(0))**2, dim=1))\n",
    "                min_idx = torch.argmin(dist)\n",
    "                score_tmp.append(goal_scores[min_idx])\n",
    "            score_pred.append(score_tmp)\n",
    "        score_pred = torch.Tensor(score_pred).to(device)\n",
    "\n",
    "    return goda_cls, traj_pred, goal_pred, score_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 528 is out of bounds for dimension 0 with size 528",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8408/62767243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgoda_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_goal_and_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_idcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ctrs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding: goda_cls:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoda_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding: traj_pred:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraj_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding: goal_pred:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoding: score_pred:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8408/2092149432.py\u001b[0m in \u001b[0;36mdecode_goal_and_traj\u001b[0;34m(feat_da, graph_da, actors, actor_idcs, goal_gt)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgoda_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoda_cls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_da\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idcs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midcs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mDEFAULT_GODA_NUM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Invalid goda number'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 528 is out of bounds for dimension 0 with size 528"
     ]
    }
   ],
   "source": [
    "goda_cls, traj_pred, goal_pred, score_pred = decode_goal_and_traj(actors, graph, actors, actor_idcs, data['ctrs'])\n",
    "print(\"decoding: goda_cls:\", goda_cls)\n",
    "print(\"decoding: traj_pred:\", traj_pred)\n",
    "print(\"decoding: goal_pred:\", goal_pred)\n",
    "print(\"decoding: score_pred:\", score_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('lanegcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07e1035589aed84bd7169a62ca865480aa9cea1eaec18b5fbc2b7aab1975a44f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
